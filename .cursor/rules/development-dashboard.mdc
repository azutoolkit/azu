---
description:
globs:
alwaysApply: false
---
## Claude Prompt: Azu Development Dashboard HTTP Handler

You are helping build a **Development Dashboard** HTTP handler for the [Azu web framework](mdc:https:/github.com/azutoolkit/azu), written in Crystal.

The dashboard should be rendered in HTML using Azu‚Äôs template engine (`Response.render("dev_dashboard.slang", data)`) and expose internal metrics useful to developers. It will be used during development only, with optional protection.

---

### üîÄ Goal

Implement a clean, extensible `DevDashboardHandler` class that exposes live runtime insights from the system. Use the `Azu::PerformanceMetrics` module (included below) to show fine-grained application behavior.

---

### üìä Dashboard Sections & Data Requirements

#### 1. ‚úÖ Application Status

* Uptime (`@start_time`)
* Current memory usage (`PerformanceMetrics.current_memory_usage`)
* Total requests
* Error rate (% from metrics)
* CPU usage (mock or shell out)

#### 2. ‚úÖ Database Info

* Connection status (mock or `CQL.connected?`)
* Migration status (mock)
* Table count (e.g., `CQL.tables.size`)
* Query performance (mock or extend)

#### 3. ‚úÖ Route Listing

* Use Azu route registry (`Azu::Router.routes`)
* Show method, path, handler class, route params

#### 4. ‚úÖ Performance Metrics

Use `PerformanceMetrics.aggregate_stats` and related methods:

* Avg response time, P95, P99
* Memory allocation delta
* Request throughput (req/sec since start)
* GC stats (mock or from Crystal GC API)

#### 5. ‚úÖ Cache Metrics

Combine from `cache_stats`, `cache_operation_breakdown`, local `cache.stats`, and optionally use `JsonPerformanceReporter`:

* Total operations, hit rate, avg time
* Operation breakdown (`get`, `set`, `delete`, etc.)
* Error rates
* Data written
* Size of cache store
* Max configured size
* In-memory usage in MB (`memory_usage_mb`)
* Hit rate via `calculate_hit_rate`

#### 6. ‚úÖ Component Lifecycle

(from `component_stats`):

* Mount/unmount/refresh counts
* Avg component lifespan
* Memory deltas

#### 7. ‚úÖ Error Logs

* Most recent errors (`recent_requests.select(&.error?)`)
* Show stack trace if available (mocked if not)
* Categorize by 4xx/5xx

#### 8. ‚úÖ Test Results

* Mock with JSON/env:

  * Last test run
  * Code coverage %
  * Failed tests
  * Test suite time

---

### üîí Requirements

* All metric logic uses `Azu::PerformanceMetrics` API
* HTML rendering via `dev_dashboard.slang`
* Modular helpers like `collect_request_stats`
* Return `Azu::Response` in `call`
* Optional: `?clear=true` clears metrics via `metrics.clear`
* JSON performance report generation may reuse `JsonPerformanceReporter.generate_report`

---

### üîç Output Required

```crystal
class DevDashboardHandler < Azu::Handler
  def call(request : HTTP::Request) : Azu::Response
    # Collect all data and render HTML
  end
end
```

Use mock data where needed. Suggest enhancements for visualization, aggregation, or DX improvements.

---

> Bonus: Propose extensions such as live polling, JSON API endpoint for metrics, or charts via JS (e.g. Chart.js) if applicable.

